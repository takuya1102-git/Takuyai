cnn_v1
	結果：0.6930 	0.5060 	0.6937 	0.4840 
	寸評：全くダメ　testnormalmodel dog50%しか出ない。
	修正：①Adam＝(lr=0.001)に変更　②epochs=50へ変更　③model.add(Dropout(0.3))へ変更 ③'もあり　理由：GEMINI

cnn_v2
	変更：①Adam＝(lr=0.001)に変更
	変更：②epochs=50へ変更
	変更：③model.add(Dropout(0.3))へ変更 ③'もあり
	結果：0.0254 	0.9933 	2.4917 	0.5960 	cat99	dog63	cat85
	寸評：epochsを20回増やしfitresultは最高結果。しかしevaluateresult accuracyは1ポイント上がって成果だが、lossも大幅に上がり2.6を付ける。
	過学習の疑い。正答率も1/3と変わらず。
	修正：④epochs=15 理由：以降の結果向上は見られず、過学習を防ぐ為。
	
	gemini:ニューラルネットワークモデルが50エポックの学習を終え、トレーニングデータセットに対する損失が0.0254、正解率が99.33%と		なったこ	とを示しています。また、テストデータセットに対する評価結果として、損失が2.4917、正解率が59.60%となったことを示しています。
	これらの結果から、モデルはトレーニングデータセットに対しては非常に高い性能を示していますが、テストデータセットに対しては過学習を起こして	いる可能性が考えられます。過学習とは、モデルがトレーニングデータセットに過度に適合しすぎてしまい、新しいデータに対してうまく汎化できない	現象を指します。
	この問題を解決するために、以下のような対策が考えられます。	
	データの増強: トレーニングデータセットを増やすことで、モデルがより多くのパターンを学習し、過学習を防ぐことができます。
	正則化: モデルの複雑さを制限する手法です。L1正則化やL2正則化などがよく用いられます。
	早期停止: モデルの性能が改善しなくなった時点で学習を停止することで、過学習を防ぐことができます。

cnn_v3
	変更：④epochs=15 理由：以降の結果向上は見られず、過学習を防ぐ為。
	結果：0.1977 	0.9172 	0.9389 	0.6800 	cat88	cat57	dog92
	寸評：評価結果は前回より向上したが、推論では1/3と結果変わらず。
	修正：⑤SGD(lr=0.001)へ変更する。　理由：小規模データセットの為。
cnn_v4
	変更：⑤SGD(lr=0.001)へ変更する。　小規模データセットの為。
	結果：0.6825 	0.5621 	0.6852 	0.5800 
	寸評：ｖ1と変わらない性能　推論　1/3
	修正：⑥momentumへ変更　理由：⑤と比較
	
cnn_v5
	変更：⑥momentumへ変更　理由：⑤と比較
	結果：
	寸評：
	修正：⑦L1正則化を導入
		****コード******
	from sklearn.linear_model import Lasso
	from sklearn.datasets import load_diabetes

	# データの読み込み
	X, y = load_diabetes(return_X_y=True)

	# L1正則化（Lasso）モデルの作成
	lasso = Lasso(alpha=0.1)  # alphaは正則化の強さを制御するハイパーパラメータ

	# モデルの学習
	lasso.fit(X, y)

	# 係数を出力
	print(lasso.coef_)
	*****************
	
cnn_v6
	変更：⑦L1正則化を導入

	
確かに、現在のモデルの性能 (loss: 0.6930、accuracy: 0.5060) を向上させる余地がありそうです。ここでは、以下の点に着目してコードを修正してみましょう。

**1. ハイパーパラメータのチューニング**

* 学習率 (lr): 0.01  今の学習率は少し高いかもしれません。**0.001** や **0.0001** など、小さい値を試してみましょう。
* バッチサイズ (batch_size): 32 バッチサイズは妥当な値ですが、**16** や **64** などを試してみるのも良いでしょう。
* エポック数 (epochs): 30epochs までは訓練が不足している可能性があります。**50** や **100** まで増やしてみましょう。
* ドロップアウト率 (Dropout): 0.25, 0.5 今の値でも良いですが、**0.1** や **0.3** など他の値を試してみることもできます。

**2. モデルアーキテクチャの改良**

* 畳み込み層 (Conv2D) のフィルター数やカーネルサイズを変更する。
* プーリング層 (MaxPooling2D) のプーリングサイズを変更する。
* バッチ正規化 (BatchNormalization) を追加する。
* レイヤーを増やす (ただし複雑になりすぎるのを避ける)

**3. データ拡張 (Data Augmentation)**

* 画像を回転、拡大縮小、水平反転させるなどして、訓練データを増やす。

修正コードの一例として、学習率を下げて、エポック数を増やすようにしてみましょう。

```python
# ... (他のコードは同じ)

# ハイパーパラメータのチューニング
opt = Adam(lr=0.001)  # 学習率を下げる

model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])

model.fit(X_train, y_train, batch_size=32, epochs=50)  # エポック数を増やす

score = model.evaluate(X_test, y_test, batch_size=32)
model.save("./cnn.h5")
```

**注意** 上記のコードは一例であり、最適なハイパーパラメータやモデルアーキテクチャはデータセットによって異なります。グリッドサーチやランダムサーチなどの手法を使って、**様々な設定を試して、一番良い結果が出るように検証**してみてください。

また、データ拡張を取り入れることで、学習データのバリエーションを増やし、過学習を抑制することができる可能性があります。

試行錯誤を繰り返しながら、モデルの性能向上を目指してください。
